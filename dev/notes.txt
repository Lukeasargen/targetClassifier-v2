cd projects
cd targetClassifier-v2
conda activate uas_vision

tensorboard --logdir logs/
--------------------------------------------------------------------------------
Old dataloader
11.50 seconds with 0 workers. 0.72 seconds per batch. 0.007 GB ram.
12.59 seconds with 1 workers. 0.79 seconds per batch. 0.326 GB ram.
7.95 seconds with 2 workers. 0.50 seconds per batch. 0.643 GB ram.
7.72 seconds with 3 workers. 0.48 seconds per batch. 0.968 GB ram.
7.40 seconds with 4 workers. 0.46 seconds per batch. 1.291 GB ram.
8.31 seconds with 5 workers. 0.52 seconds per batch. 1.607 GB ram.
8.78 seconds with 6 workers. 0.55 seconds per batch. 1.922 GB ram.
9.79 seconds with 7 workers. 0.61 seconds per batch. 2.226 GB ram.
10.45 seconds with 8 workers. 0.65 seconds per batch. 2.528 GB ram.

New dataloader - 4x faster with 8 workers, and it uses way less memory
12.40 seconds with 0 workers. 0.77 seconds per batch. 0.514 GB ram.
12.84 seconds with 1 workers. 0.80 seconds per batch. 0.569 GB ram.
6.83 seconds with 2 workers. 0.43 seconds per batch. 0.543 GB ram.
5.16 seconds with 3 workers. 0.32 seconds per batch. 0.552 GB ram.
3.64 seconds with 4 workers. 0.23 seconds per batch. 0.566 GB ram.
3.65 seconds with 5 workers. 0.23 seconds per batch. 0.589 GB ram.
2.97 seconds with 6 workers. 0.19 seconds per batch. 0.584 GB ram.
3.07 seconds with 7 workers. 0.19 seconds per batch. 0.562 GB ram.
2.43 seconds with 8 workers. 0.15 seconds per batch. 0.634 GB ram.
--------------------------------------------------------------------------------
python classify.py 
--seed=42 --name=classify --workers=4 --gpus=0

--img_size=32 --min_size=28 --alias_factor=1 --fill_prob=0.5 
--backgrounds=images/backgrounds

dataset, no backgrounds, classify stats
--mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862
half google and half dota backgrounds
--mean 0.43949696 0.4287838  0.3273663 --std 0.18663725 0.1653652  0.18666378

--feature_dim=64 --dropout=0.2

--train_size=320 --batch=32 --epochs=1

--opt=sgd --lr=6e-2 --weight_decay=1e-5 --momentum=0.9 --nesterov
--opt=adam --lr=4e-3
--opt=adamw --lr=4e-3 --weight_decay=5e-4
--lr_warmup_steps=0
--accumulate=1
--grad_clip=value --clip_value=0.0

--scheduler=step --milestones 10 15 --lr_gamma=0.1
--scheduler=exp --lr_gamma=0.95
--scheduler=plateau --plateau_patience=20 --lr_gamma=0.2 --min_lr=1e-6

--weighted_loss
--------------------------------------------------------------------------------
todo
better model
resnet backbone
nfnet stem, regnet Y
anti alias pooling
gelu

saptial transformer to crop
compare ST features to main features, add projection heads

stop training task after threshold, (eg >99% has_target)
set max size in load_backgrounds to resize and save space

--------------------------------------------------------------------------------
test commands

v27 - no schedule
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=1 --fill_prob=0.5 --mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862 --feature_dim=128 --dropout=0.2 --train_size=2560 --batch=128 --epochs=500 --opt=sgd --lr=6e-2 --weight_decay=1e-5 --momentum=0.9 --nesterov --lr_warmup_steps=500
v28 - no schedule, adamw
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=1 --fill_prob=0.5 --mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862 --feature_dim=128 --dropout=0.2 --train_size=2560 --batch=128 --epochs=500 --opt=adamw --lr=4e-3 --weight_decay=1e-5 --lr_warmup_steps=500
v29 - step --scheduler=step --milestones 300 450 --lr_gamma=0.1
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=1 --fill_prob=0.5 --mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862 --feature_dim=128 --dropout=0.2 --train_size=2560 --batch=128 --epochs=500 --opt=sgd --lr=6e-2 --weight_decay=1e-5 --momentum=0.9 --nesterov --lr_warmup_steps=500 --scheduler=step --milestones 300 450 --lr_gamma=0.1
v30 - exp --scheduler=exp --lr_gamma=0.99
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=1 --fill_prob=0.5 --mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862 --feature_dim=128 --dropout=0.2 --train_size=2560 --batch=128 --epochs=500 --opt=sgd --lr=6e-2 --weight_decay=1e-5 --momentum=0.9 --nesterov --lr_warmup_steps=500 --scheduler=exp --lr_gamma=0.99
v31 - plateau --scheduler=plateau --plateau_patience=50 --lr_gamma=0.5
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=1 --fill_prob=0.5 --mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862 --feature_dim=128 --dropout=0.2 --train_size=2560 --batch=128 --epochs=500 --opt=sgd --lr=6e-2 --weight_decay=1e-5 --momentum=0.9 --nesterov --lr_warmup_steps=500 --scheduler=plateau --plateau_patience=50 --lr_gamma=0.5

python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=2 --fill_prob=0.5 --mean 0.5368893  0.43507764 0.3849545 --std 0.13682073 0.13410869 0.14616862 --feature_dim=256 --dropout=0.25 --train_size=2560 --batch=128 --epochs=2000 --opt=sgd --lr=6e-2 --weight_decay=1e-5 --momentum=0.9 --nesterov --lr_warmup_steps=500 --scheduler=plateau --plateau_patience=50 --lr_gamma=0.5 --min_lr=1e-6

python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=2 --fill_prob=0.5 --backgrounds=C:\Users\lukeasargen\projects\aerial_backgrounds --mean 0.43949696 0.4287838  0.3273663 --std 0.18663725 0.1653652  0.18666378 --feature_dim=128 --dropout=0.0 --train_size=16384 --batch=256 --epochs=72 --opt=sgd --lr=6e-2 --weight_decay=5e-4 --momentum=0.9 --nesterov --lr_warmup_steps=1472 --scheduler=step --milestones 62 68 --lr_gamma=0.1 --weighted_loss
v2 - no weight
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=2 --fill_prob=0.5 --backgrounds=C:\Users\lukeasargen\projects\aerial_backgrounds --mean 0.43949696 0.4287838  0.3273663 --std 0.18663725 0.1653652  0.18666378 --feature_dim=128 --dropout=0.0 --train_size=3200 --batch=128 --epochs=100 --opt=sgd --lr=4e-2 --weight_decay=1e-4 --momentum=0.9 --nesterov --lr_warmup_steps=1000 --scheduler=step --milestones 80 90 --lr_gamma=0.1
v3 - weighted
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=2 --fill_prob=0.5 --backgrounds=C:\Users\lukeasargen\projects\aerial_backgrounds --mean 0.43949696 0.4287838  0.3273663 --std 0.18663725 0.1653652  0.18666378 --feature_dim=128 --dropout=0.0 --train_size=3200 --batch=128 --epochs=100 --opt=sgd --lr=4e-2 --weight_decay=1e-4 --momentum=0.9 --nesterov --lr_warmup_steps=1000 --scheduler=step --milestones 80 90 --lr_gamma=0.1 --weighted_loss

v4 - v3 but multiply by sigma also
v5 - v2 with dropout=0.25

v6 - long training to see how the tasks improve
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=32 --min_size=28 --alias_factor=2 --fill_prob=0.5 --backgrounds=C:\Users\lukeasargen\projects\aerial_backgrounds --mean 0.43949696 0.4287838  0.3273663 --std 0.18663725 0.1653652  0.18666378 --feature_dim=256 --dropout=0.1 --train_size=12800 --batch=128 --epochs=250 --opt=sgd --lr=5e-2 --weight_decay=1e-4 --momentum=0.9 --nesterov --lr_warmup_steps=1000 --scheduler=step --milestones 150 200 --lr_gamma=0.1

v7 - increase input size to 40
python classify.py --seed=42 --name=classify --workers=4 --gpus=0 --img_size=40 --min_size=28 --alias_factor=2 --fill_prob=0.5 --backgrounds=C:\Users\lukeasargen\projects\aerial_backgrounds --mean 0.43949696 0.4287838  0.3273663 --std 0.18663725 0.1653652  0.18666378 --feature_dim=256 --dropout=0.1 --train_size=12800 --batch=128 --epochs=200 --opt=sgd --lr=4e-2 --weight_decay=1e-4 --momentum=0.9 --nesterov --lr_warmup_steps=1000 --scheduler=step --milestones 170 190 --lr_gamma=0.1

large input hurt only the letter and shape accuracy for this model


after this actually make a better model

